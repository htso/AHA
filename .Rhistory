cat("whitening completed.\n")
}
# 4. use k-means to get the matrix of centroid centers ===================================
runtime = system.time(km <- kmeans(Y.wh[sample(nrow(Y.wh), nrow(Y.wh), replace=FALSE),],
centers=K, nstart=1, algorithm="Lloyd", iter.max=1600))
KM.CTR = km$centers # K x w*w*3
if ( verbose ) {
cat("kmeans runtime :", runtime, "\n")
save(KM.CTR, K, Y.wh, Mu, Sigm, w, U, ev, file="step4-kmeans.RData")
}
param = list(mu=Mu, sigm=Sigm, U=U, ev=ev, km.ctr=KM.CTR)
return(param)
}
create_RBM_learner = function(data, nHidden, rseed=17, verbose=FALSE) {
require(rbm)
param = list()
return(param)
}
extract_features = function(data, w, stride, learner, hard ) {
require(Rcpp)
feat = cpp_Conv_Fea(data, w, stride,
learner$mu,
learner$sigm,
t(learner$U),
learner$ev,
learner$km.ctr,
hard)
return(feat)
}
# w : subpatch size, assumed to be a square
# s : stride, or step side of the sliding window
# K : no of features for classification
# N_patches : number of subpatches to extract from raw image
# C.heur : the cost parameter for SVM
# svm.type : SVM type required by LiblineaR
# verbose : print diagnostics and save intermediary results
train_by_kmeans = function(trainset, lbl.train, features, C.heur=NULL, svm.type=2, verbose=FALSE) {
require(LiblineaR)
# Second normalization to prepare for SVM ====================
data.scaled = applyScaling2Mat( features, scale.f=NULL, method=2 )
Indat.s = data.scaled[["data.s"]]
if ( verbose )
cat("second normalization done.\n")
# Use heuristics to estimate the "Cost" of SVM ==========================================
if ( is.null(C.heur) == TRUE ) {
runtime = system.time(C.heur <- heuristicC(Indat.s[sample(nrow(Indat.s), 40000, replace=FALSE),]))
if ( verbose ) {
cat("SVM search heuristic done. runtime :", runtime, "\n")
cat("Cost chosen by heuristic :", C.heur, "\n")
}
}
# Fast linear SVM ======================================================
runtime = system.time(lsvm <- LiblineaR(data=Indat.s, target=lbl.train,
type=svm.type,
wi=NULL,
cost=C.heur,
cross=0,
verbose=TRUE))
if ( verbose ) {
cat("LiblineaR runtime :", runtime, "\n")
save(lsvm, C.heur, lbl.train, data.scaled, file="step9.RData")
}
return(list(lsvm=lsvm, scale.f=data.scaled[["scale.f"]]))
}
predict_svm = function(mod, data) {
return( as.integer(predict(mod, data)$predictions) )
}
calc_accuracy = function(pred, act) {
return( 100*sum(pred == factor(act))/length(pred) )
}
tmp = readCifar10(dat_dir)
dat = prepare_Cifar10(tmp)
home_dir = "/home/rspace/RepulseBay/Rhome/CoatesHonglakNg/AHA"
setwd(home_dir)
sourceCpp("Cpp-scripts/Cpp-for-Conv105.cpp")
trainset = dat[["train"]]
testset = dat[["test"]]
lbl.train = dat[["lbl.tr"]]
lbl.test = dat[["lbl.tt"]]
w = 6
stride = 1
K = 100
N_patches = 10000
system.time(learner <- create_kmeans_learner(data=trainset, K=K, N_patches=N_patches, w=w, stride=stride, rseed=rseed, verbose=verbose))
# ConvFun.R
# Horace W. Tso
# Oct ?, 2014
# Ref : Adam Coates, Honglak Lee, Andrew Ng, An analysis of single-layer networks
#       in unsupervised feature learning, Intl Conf on Artificial Intelligence & Stat,
#       2011.
library(pixmap)
# dat : CIFAR data matrix where each row is one image, and each column consists of the
#       red, green blue color channel, each channel is a 32 x 32 matrix
# N_patches : number of random subpatch to generate
# w : the width of a square subpatch
# stride : the step size of the subpatch, assumed to be 1 for now
random.cifar.subpatches = function(dat, N_patches, w, stride=1, seed=17) {
set.seed(seed)
NN = nrow(dat)
N = sqrt(ncol(dat) / 3)
# red first, then green and blue
ir = 1:(N*N)
ig = (N*N+1):(N*N*2)
ib = (N*N*2+1):(N*N*3)
nr = (N - w)/stride + 1
nc = (N - w)/stride + 1
Y = matrix(NA, nrow=N_patches, ncol=w*w*3)
k = 1
while( k <= N_patches ) {
u = sample(NN, 1)
# dat is arranged by row-major convention, thus converting back
# to a 2D matrix needs to set byrow=TRUE
rmat = matrix(dat[u, ir], nrow=N, byrow=TRUE)
gmat = matrix(dat[u, ig], nrow=N, byrow=TRUE)
bmat = matrix(dat[u, ib], nrow=N, byrow=TRUE)
i = sample(nr, 1)
j = sample(nc, 1)
# Collapsing a submatrix back to a smaller vector
# NOTE : transpose is needed to conform to row-major convention in dat
rpatch = as.vector(t(rmat[i:(i+w-1), j:(j+w-1)]))
gpatch = as.vector(t(gmat[i:(i+w-1), j:(j+w-1)]))
bpatch = as.vector(t(bmat[i:(i+w-1), j:(j+w-1)]))
Y[k,] = c(rpatch, gpatch, bpatch)
k = k + 1
}
return(Y)
}
# dat : CIFAR data matrix where each row is one image, and each column consists of the
#       red, green blue color channel, each channel is a 32 x 32 matrix
# K : number of K-means centroid
# N : the wide and height of the image
# w : the width of a square subpatch
# s : the step size of the subpatch, assumed to be 1 for now
# KM.CTR : the kmeans cluster centers
# Mu : vector of means of the subpatch vector, size w*w*3
# Sigm : vector of the standard deviations of the subpatch vector, size w*w*3
# U : the matrix formed by the eigenvectors of PCA
# ev : vector of the sqrt of the eigenvalues of the covariance matrix
Conv.pool = function(dat,  K, N, w, s=1, KM.CTR, Mu, Sigm, U, ev) {
NN = nrow(dat)
# no of steps by horizontal and vertical direction
nr = (N - w)/s + 1
nc = (N - w)/s + 1
# useful indices to access the 3-color channels in dat
ir = 1:(32*32)
ig = (32*32+1):(32*32*2)
ib = (32*32*2+1):(32*32*3)
Fea.pool = matrix(NA, ncol=4*K+1, nrow=NN)
# Fea.pool : 50000 x 4*K+1
for ( u in 1:NN ) { # go thru each row (image)
# reconstruct the original image by turning a row into a 2D matrix
rm = matrix(dat[u, ir], nrow=N, byrow=TRUE)
gm = matrix(dat[u, ig], nrow=N, byrow=TRUE)
bm = matrix(dat[u, ib], nrow=N, byrow=TRUE)
# for each subpatch, normalize and whiten the data, then calculate the
# distance of the K centroids, which produces a K-vector.
# Keep in mind that stride is assumed to be 1
patch.Fea = array(NA, dim=c(nr, nc, K))
for ( i in 1:nr ) {
for ( j in 1:nc ) {
# NOTE : transpose is needed to collapse them into vector by the row-major convention
rv = as.vector(t(rm[i:(i+w-1), j:(j+w-1)]))
gv = as.vector(t(gm[i:(i+w-1), j:(j+w-1)]))
bv = as.vector(t(bm[i:(i+w-1), j:(j+w-1)]))
# concatenate into a vector of length w*w*3
vv = c(rv, gv, bv)
# normalize the vector by the given mean and st dev
v.norm = (vv - Mu) / Sigm
# whiten the vector by the given U and ev
v.wh = (t(U) %*% v.norm) / ev
#patch.Fea[i,j,] = cpp_triangle_kmeans(c(v.wh), KM.CTR)
patch.Fea[i,j,] = cpp_dist_from_centroids(c(v.wh), KM.CTR)
}
}
# pooling in the four quadrants =================================
# |-------------|
# |      |      |
# |  I   |  II  |
# |      |      |
# |-------------|
# |      |      |
# |  IV  |  III |
# |      |      |
# |-------------|
Rmid = floor(nr / 2)
Cmid = floor(nc / 2)
# Quadrant I :
Pool.I = double(K)
for ( ii in 1:Rmid ) {
for ( jj in 1:Cmid ) {
Pool.I = Pool.I + patch.Fea[ii,jj,]
}
}
# Quadrant II :
Pool.II = double(K)
for ( ii in 1:Rmid ) {
for ( jj in (Cmid+1):nc ) {
Pool.II = Pool.II + patch.Fea[ii,jj,]
}
}
# Quadrant III :
Pool.III = double(K)
for ( ii in (Rmid+1):nr ) {
for ( jj in (Cmid+1):nc ) {
Pool.III = Pool.III + patch.Fea[ii,jj,]
}
}
# Quadrant IV :
Pool.IV = double(K)
for ( ii in (Rmid+1):nr ) {
for ( jj in 1:Cmid ) {
Pool.IV = Pool.IV + patch.Fea[ii,jj,]
}
}
# Just concatenate the four K-vector to form a 4K feature
# return(c(u, Pool.I, Pool.II, Pool.III, Pool.IV))
Fea.pool[u,] = c(u, Pool.I, Pool.II, Pool.III, Pool.IV)
}
return(Fea.pool)
}
# FUNCTIONS ==============================================================
# x : matrix where rows are observations and columns the features
normize = function(x) {
ctr = apply(x, 2, mean)
stds = apply(x, 2, sd)
x1 = t(t(x) - ctr)
x2 = t(t(x1) / stds)
return(list(x=x2, ctr=ctr, stds=stds))
}
# x : matrix where rows are observations and columns the features
whiten = function(x) {
pp = prcomp(x, center=FALSE, scale=FALSE, retx=TRUE)
U = pp$rotation # U's columns are the eigenvectors
ev.sqrt = pp$sdev # these are sqrt of eigenvalues
x.rot = t(U) %*% t(x) # (3072 x 3072) x t(5000 x 3072) = 3072 x 5000
x.rot = t(x.rot)
x.wh =t(t(x.rot) / ev.sqrt )
return(list(x=x, U=U, ev=ev.sqrt))
}
# x : vector of features
# ctr, stds : the center vector and stdev vector provided
normize.against = function(x, ctr, stds) {
return(c((x-ctr)/stds))
}
# x : matrix where rows are observations and columns the features
# U, ev : the matrix of eigenvectors and square root of eigenvalues provided
whiten.against = function(x, U, ev) {
x.rot = t(U) %*% x # (3072 x 3072) x t(5000 x 3072) = 3072 x 5000
return(x.rot / ev )
}
# patch : 1 x N matrix, where there are M rows of image patches, each patch has N columns
# cent : K x N matrix of centroids
# calculate the euclidean distance between each row of patch with each row of cent.
# Ref : Coates, Lee, Ng, Analysis of Single-Layer Networks in Unsupervised Feature Learning, ICML 2011
# fast version
triangle.Kmeans1 = function(patch, cent) {
n = nrow(cent)
z.k = NULL
for ( i in 1:n ) z.k[i] = sqrt(sum((cent[i,] - patch)*(cent[i,] - patch)))
mu.z = mean(z.k)
z1 = - z.k + mu.z
return(pmax(0,z1))
}
# slow version
triangle.Kmeans0 = function(patch, cent) {
n = nrow(cent)
# z.k : M x K
z.k = t(apply(patch, 1, function(y) apply(cent, 1, function(r) norm(r-y,"2"))))
mu.z = apply(z.k, 1, mean) # M x 1
z1 = t(- t(z.k) + mu.z) # M x K
f.k = matrix(pmax(0, z1), ncol=ncol(z1))
return(f.k)
}
# Ref : Coates, Lee, Ng, Analysis of Single-Layer Networks in Unsupervised Feature Learning, ICML 2011
hard.Kmeans = function(patch, cent) {
# z.k : M x K
z.k = t(apply(patch, 1, function(y) apply(cent, 1, function(r) norm(r-y,"2"))))
ix = apply(z.k, 1, which.min)
f.k = matrix(0, nrow=nrow(z.k), ncol=ncol(z.k))
for ( i in 1:nrow(f.k)) {
f.k[i,ix[i]] = 1
}
return(f.k)
}
# Parallel version of Conv.pool
# NOT USED
extract.pool.fea = function(dat, K, N, w, s=1, km.ctr, Mu, Sigm, U, ev, ncore=11) {
require(foreach)
require(doSNOW)
clus = makeCluster(ncore, type="SOCK")
registerDoSNOW(clus)
NN = nrow(dat)
nr = (N - w)/s + 1
nc = (N - w)/s + 1
Rmid = floor(nr / 2)
Cmid = floor(nc / 2)
# useful indices
ir = 1:(32*32)
ig = (32*32+1):(32*32*2)
ib = (32*32*2+1):(32*32*3)
Fea.pool <- foreach ( u=1:NN, .combine=rbind )  %dopar%
{
# reconstruct the original image by turning a row into a 2D matrix
r1 = matrix(dat[u, ir], nrow=N, byrow=TRUE)
g1 = matrix(dat[u, ig], nrow=N, byrow=TRUE)
b1 = matrix(dat[u, ib], nrow=N, byrow=TRUE)
# pick out the subpatches (stride=1)
patch.K = array(NA, dim=c(nr, nc, K))
for ( i in 1:nr ) {
for ( j in 1:nc ) {
rv = as.vector(r1[i:(i+w-1), j:(j+w-1)])
gv = as.vector(g1[i:(i+w-1), j:(j+w-1)])
bv = as.vector(b1[i:(i+w-1), j:(j+w-1)])
vmat = matrix(c(rv, gv, bv), nrow=1)
v.norm = normize.against(vmat, Mu, Sigm)
v.wh = whiten.against(v.norm, U, ev)
# Use the soft k-means to extract features from image
patch.K[i,j,] = triangle.Kmeans(v.wh, km.ctr)
}
}
# Quadrant I :
Pool.I = double(K)
for ( ii in 1:Rmid ) for ( jj in 1:Cmid ) Pool.I = Pool.I + patch.K[ii,jj,]
# Quadrant II :
Pool.II = double(K)
for ( ii in 1:Rmid ) for ( jj in (Cmid+1):nc ) Pool.II = Pool.II + patch.K[ii,jj,]
# Quadrant III :
Pool.III = double(K)
for ( ii in (Rmid+1):nr ) for ( jj in (Cmid+1):nc ) Pool.III = Pool.III + patch.K[ii,jj,]
# Quadrant IV :
Pool.IV = double(K)
for ( ii in (Rmid+1):nr ) for ( jj in 1:Cmid ) Pool.IV = Pool.IV + patch.K[ii,jj,]
# Just concatenate the four K-vector to form a 4K feature
return(c(u, Pool.I, Pool.II, Pool.III, Pool.IV))
}
stopCluster(clus)
return(Fea.pool)
}
system.time(learner <- create_kmeans_learner(data=trainset, K=K, N_patches=N_patches, w=w, stride=stride, rseed=rseed, verbose=verbose))
rseed = 1701
system.time(learner <- create_kmeans_learner(data=trainset, K=K, N_patches=N_patches, w=w, stride=stride, rseed=rseed, verbose=verbose))
10^5
10*10*10*10*10
names(lerner)
names(learner)
str(learner)
K
system.time(train_features <- extract_features(trainset, w, stride, learner, 0))
system.time(res <- train_by_kmeans(trainset, lbl.train, train_features, C.heur=NULL, svm.type=2, verbose=FALSE))
system.time(test.features <- extract_features(testset, w, stride, learner, 0))
scale.f = res[["scale.f"]]
testset <- applyScaling2Mat(test.features, scale.f, 2)
datt = testset[["data.s"]]
mod = res[["lsvm"]]
pred = predict_svm(mod, datt)
system.time(test.features <- extract_features(testset, w, stride, learner, 0))
head(pred)
class(pred)
dim(testset)
class(testset)
names(testset)
calc_accuracy(pred, lbl.test)
table(pred, lbl.test)
trainset = dat[["train"]]
testset = dat[["test"]]
lbl.train = dat[["lbl.tr"]]
lbl.test = dat[["lbl.tt"]]
w = 6
stride = 1
K = 200
N_patches = 20000
rseed = 1701
system.time(learner <- create_kmeans_learner(data=trainset, K=K, N_patches=N_patches, w=w, stride=stride, rseed=rseed, verbose=verbose))
system.time(train_features <- extract_features(trainset, w, stride, learner, 0))
system.time(res <- train_by_kmeans(trainset, lbl.train, train_features, C.heur=NULL, svm.type=2, verbose=FALSE))
system.time(test.features <- extract_features(testset, w, stride, learner, 0))
scale.f = res[["scale.f"]]
tfea.norm <- applyScaling2Mat(test.features, scale.f, 2)
datt = tfea.norm[["data.s"]]
mod = res[["lsvm"]]
pred = predict_svm(mod, datt)
calc_accuracy(pred, lbl.test)
table(pred, lbl.test)
library(AHA)
getwd()
library(AHA)
data(CIFAR10)
ls()
dim(cifar1)
str(cifar1)
getwd()
system.files(package="AHA")
system.file(package="AHA")
dat = rbind(cifar1$data, cifar2$data, cifar3$data, cifar4$data, cifar5$data)
lbl.In = c(cifar1$labels, cifar2$labels, cifar3$labels, cifar4$labels, cifar5$labels)
datt = cifart$data
lbl.Out = c(cifart$labels)
dim(dat)
dat1 =dat[sample(nrow(dat), 5000, replace=FALSE),]
dim(dat1)
ix = sample(nrow(dat), 5000, replace=FALSE)
head(ix, 100)
duplicated(ix)
sum(duplicated(ix))
dat1 =dat[ix,]
ls()
length(lbl.In)
lbl.In1 = lbl.In[ix]
length(lbl.In1)
ix2 = sample(nrow(datt), 1000, replace=FALSE)
length(ix2)
datt1 = datt[ix2,]
lbl.Out1 = lbl.Out[ix2]
table(lbl.Out1)
table(lbl.In1)
getwd()
ls()
dat = dat1
lbl.In = lbl.In1
datt = datt1
lbl.Out = lbl.Out1
dim(dat)
dim(datt)
length(lbl.In)
length(lbl.Out)
save(dat, datt, lbl.In, lbl.Out, file="tinyCifar10.RData")
system.file(package="AHA")
library(AHA)
prepare_Cifar10
create_kmeans_learner
applyScaling2Mat
data(tinyCifar10)
ls9)
ls()
dim(dat)
sourceCpp("Cpp-scripts/Cpp-for-Conv105.cpp")
library(Rcpp)
sourceCpp("Cpp-scripts/Cpp-for-Conv105.cpp")
getwd()
library(AHA)
getwd()
data(tinyCifar10)
ls
ls()
library(AHA)
getwd()
ls()
data(tinyCifar10)
ls()
library(Rcpp)
sourceCpp("Cpp-scripts/Cpp-for-Conv105.cpp")
w = 6
stride = 1
K = 100
N_patches = 10000
rseed = 1701
system.time(learner <- create_kmeans_learner(data=dat, K=K, N_patches=N_patches, w=w, stride=stride, rseed=rseed, verbose=verbose))
system.time(learner <- create_kmeans_learner(data=dat, K=K, N_patches=N_patches, w=w, stride=stride, rseed=rseed, verbose=verbose))
verbose = TRUE
system.time(learner <- create_kmeans_learner(data=dat, K=K, N_patches=N_patches, w=w, stride=stride, rseed=rseed, verbose=verbose))
library(AHA)
library(AHA)
getwd()
library(Rcpp)
data(tinyCifar10)
sourceCpp("inst/Cpp-for-Conv105.cpp")
w = 6
stride = 1
K = 100
N_patches = 10000
rseed = 1701
verbose = TRUE
system.time(learner <- create_kmeans_learner(data=dat, K=K, N_patches=N_patches, w=w, stride=stride, rseed=rseed, verbose=verbose))
system.time(train_features <- extract_features(dat, w, stride, learner, 0))
system.time(res <- train_by_kmeans(dat, lbl.In, train_features, C.heur=NULL, svm.type=2, verbose=FALSE))
library(AHA)
library(AHA)
library(AHA)
library(Rcpp)
data(tinyCifar10)
sourceCpp("inst/Conv105.cpp")
ls()
w = 6
stride = 1
K = 100
N_patches = 10000
rseed = 1701
verbose = TRUE
system.time(learner <- create_kmeans_learner(data=dat, K=K, N_patches=N_patches, w=w, stride=stride, rseed=rseed, verbose=verbose))
system.time(train_features <- extract_features(dat, w, stride, learner, 0))
system.time(res <- train_by_kmeans(dat, lbl.In, train_features, C.heur=NULL, svm.type=2, verbose=FALSE))
system.time(test.features <- extract_features(datt, w, stride, learner, 0))
scale.f = res[["scale.f"]]
tfea.norm <- applyScaling2Mat(test.features, scale.f, 2)
datt.norm = tfea.norm[["data.s"]]
mod = res[["lsvm"]]
pred = predict_svm(mod, datt.norm)
calc_accuracy(pred, lbl.Out)
table(pred, lbl.Out)
K = 200
system.time(learner <- create_kmeans_learner(data=dat, K=K, N_patches=N_patches, w=w, stride=stride, rseed=rseed, verbose=verbose))
system.time(train_features <- extract_features(dat, w, stride, learner, 0))
system.time(res <- train_by_kmeans(dat, lbl.In, train_features, C.heur=NULL, svm.type=2, verbose=FALSE))
system.time(test.features <- extract_features(datt, w, stride, learner, 0))
scale.f = res[["scale.f"]]
tfea.norm <- applyScaling2Mat(test.features, scale.f, 2)
datt.norm = tfea.norm[["data.s"]]
mod = res[["lsvm"]]
pred = predict_svm(mod, datt.norm)
calc_accuracy(pred, lbl.Out)
table(pred, lbl.Out)
library(AHA)
